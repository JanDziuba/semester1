{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego \n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\" \n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ],
      "metadata": {
        "id": "8esP9SxN0Gy0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAh8AAYKyGaZ"
      },
      "source": [
        "# TL;DR\n",
        "\n",
        "1. In this lab scenario you will have a chance to compare performance of the classic RNN and LSTM on a toy example. \n",
        "2. This toy example will show that maintaining memory over even 20 steps is non-trivial. \n",
        "3. Finally, you will see how curriculum learning may allow to train a model on larger sequences.\n",
        "\n",
        "# Problem definition\n",
        "\n",
        "Here we consider a toy example, where the goal is to discriminate between two types of binary sequences:\n",
        "* [Type 0] a sequence with exactly one zero (remaining entries are equal to one).\n",
        "* [Type 1] a sequence full of ones,\n",
        "\n",
        "We are especially interested in the performance of the trained models on discriminating between a sequence full of ones versus a sequence with leading zero followed by ones. Note that in this case the goal of the model is to output the first element of the sequence, as the label (sequence type) is fully determined by the first element of the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w89kJcvtzNRm"
      },
      "source": [
        "#Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WyKOni1OEiS"
      },
      "source": [
        "## Importing torch\n",
        "\n",
        "Install `torch` and `torchvision`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFlmFfiPN1hI",
        "outputId": "91a1294f-f8b0-449c-de79-5035e494ef76"
      },
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRYUIgJLOa6X",
        "outputId": "b674ec20-977b-4289-c276-e8d43b765bd7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe4444cf9b0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMfnsxS4zdp9"
      },
      "source": [
        "## Understand dimensionality\n",
        "\n",
        "Check the input and output specification [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) and [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html). The following snippet shows how we can process\n",
        "a sequence by LSTM and output a vector of size `hidden_dim` after reading\n",
        "each token of the sequence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_MMm1AaObmg"
      },
      "source": [
        "hidden_dim = 5\n",
        "lstm = nn.LSTM(1, hidden_dim)  # Input sequence contains elements - vectors of size 1\n",
        "\n",
        "# create a random sequence\n",
        "sequence = [torch.randn(1) for _ in range(10)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the hidden state (including cell state)\n",
        "hidden = (torch.zeros(1, 1, 5),\n",
        "          torch.zeros(1, 1, 5))\n",
        "\n",
        "for i, elem in enumerate(sequence):\n",
        "  # we are processing only a single element of the sequence, and there\n",
        "  # is only one sample (sequence) in the batch, the third one\n",
        "  # corresponds to the fact that our sequence contains elemenents,\n",
        "  # which can be treated as vectors of size 1\n",
        "  out, hidden = lstm(elem.view(1, 1, 1), hidden)\n",
        "  print(f'i={i} out={out.detach()}')\n",
        "print(f'Final hidden state={hidden[0].detach()} cell state={hidden[1].detach()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98WXMyozh1Sr",
        "outputId": "129b9404-749d-483b-a498-bb40d759872d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i=0 out=tensor([[[-0.0675,  0.1179,  0.1081,  0.0414, -0.0341]]])\n",
            "i=1 out=tensor([[[-0.1067,  0.1726,  0.1400,  0.0902, -0.0596]]])\n",
            "i=2 out=tensor([[[-0.1148,  0.1885,  0.1956,  0.0974, -0.0840]]])\n",
            "i=3 out=tensor([[[-0.1270,  0.2031,  0.1495,  0.1249, -0.0860]]])\n",
            "i=4 out=tensor([[[-0.1281,  0.2019,  0.1810,  0.1475, -0.1027]]])\n",
            "i=5 out=tensor([[[-0.1274,  0.2060,  0.0798,  0.1330, -0.0860]]])\n",
            "i=6 out=tensor([[[-0.1318,  0.2039,  0.0997,  0.1772, -0.1011]]])\n",
            "i=7 out=tensor([[[-0.1145,  0.2008, -0.0431,  0.1051, -0.0717]]])\n",
            "i=8 out=tensor([[[-0.1289,  0.1989,  0.0515,  0.1944, -0.1030]]])\n",
            "i=9 out=tensor([[[-0.1329,  0.1920,  0.0686,  0.1772, -0.0988]]])\n",
            "Final hidden state=tensor([[[-0.1329,  0.1920,  0.0686,  0.1772, -0.0988]]]) cell state=tensor([[[-0.2590,  0.4080,  0.1307,  0.4329, -0.2895]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph3iyjw_8F30"
      },
      "source": [
        "## To implement\n",
        "\n",
        "Process the whole sequence all at once by calling `lstm` only once and check that the output is exactly the same as above (remember to initialize the hidden state the same way)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFtxYqla8bJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf357a9-6854-4775-9241-f8efb102fd02"
      },
      "source": [
        "# #########################################################\n",
        "#                    To implement\n",
        "# #########################################################\n",
        "\n",
        "# make the sequence a tensor of correct size\n",
        "sequence = torch.tensor(sequence).view(10, 1, 1)\n",
        "\n",
        "# initialize the hidden state (including cell state)\n",
        "hidden = (torch.zeros(1, 1, 5),\n",
        "          torch.zeros(1, 1, 5))\n",
        "\n",
        "output, (final_hidden_state, final_cell_state) = lstm(sequence, hidden)\n",
        "for i, elem in enumerate(output):\n",
        "  print(f'i={i} out={elem.detach()}')\n",
        "print(f'Final hidden state={final_hidden_state.detach()} cell state={final_cell_state.detach()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i=0 out=tensor([[-0.0675,  0.1179,  0.1081,  0.0414, -0.0341]])\n",
            "i=1 out=tensor([[-0.1067,  0.1726,  0.1400,  0.0902, -0.0596]])\n",
            "i=2 out=tensor([[-0.1148,  0.1885,  0.1956,  0.0974, -0.0840]])\n",
            "i=3 out=tensor([[-0.1270,  0.2031,  0.1495,  0.1249, -0.0860]])\n",
            "i=4 out=tensor([[-0.1281,  0.2019,  0.1810,  0.1475, -0.1027]])\n",
            "i=5 out=tensor([[-0.1274,  0.2060,  0.0798,  0.1330, -0.0860]])\n",
            "i=6 out=tensor([[-0.1318,  0.2039,  0.0997,  0.1772, -0.1011]])\n",
            "i=7 out=tensor([[-0.1145,  0.2008, -0.0431,  0.1051, -0.0717]])\n",
            "i=8 out=tensor([[-0.1289,  0.1989,  0.0515,  0.1944, -0.1030]])\n",
            "i=9 out=tensor([[-0.1329,  0.1920,  0.0686,  0.1772, -0.0988]])\n",
            "Final hidden state=tensor([[[-0.1329,  0.1920,  0.0686,  0.1772, -0.0988]]]) cell state=tensor([[[-0.2590,  0.4080,  0.1307,  0.4329, -0.2895]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VAUJUhx9EAC"
      },
      "source": [
        "## Training a model\n",
        "\n",
        "Below we define a very simple model, which is a single layer of LSTM, where the output in each time step is processed by relu followed by a single fully connected layer, the output of which is a single number. We are going\n",
        "to use the number generated after reading the last element of the sequence,\n",
        "which will serve as the logit for our classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qncNRKMNOh9h"
      },
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(1, self.hidden_dim)\n",
        "        self.hidden2label = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        sequence_len = x.shape[0]\n",
        "        logits = self.hidden2label(F.relu(out[-1].view(-1)))\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF4r0AUu9g6a"
      },
      "source": [
        "Below is a training loop, where we only train on the two hardest examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTaCFMBBOkOp"
      },
      "source": [
        "SEQUENCE_LEN = 10\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "\n",
        "def eval_on_hard_examples(model):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "        for sequence in HARD_EXAMPLES:\n",
        "            input = torch.tensor(sequence[0]).view(-1, 1, 1)\n",
        "            logit = model(input)\n",
        "            logits.append(logit.detach())\n",
        "        print(f'Logits for hard examples={logits}')\n",
        "\n",
        "\n",
        "def train_model(hidden_dim, lr, num_steps=10000):\n",
        "    model = Model(hidden_dim=hidden_dim)\n",
        "    loss_function = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "\n",
        "    for step in range(num_steps):  \n",
        "        if step % 100 == 0:\n",
        "            eval_on_hard_examples(model)\n",
        "\n",
        "        for sequence, label in HARD_EXAMPLES:\n",
        "            model.zero_grad()\n",
        "            logit = model(torch.tensor(sequence).view(-1, 1, 1))  \n",
        "            \n",
        "            loss = loss_function(logit.view(-1), torch.tensor([label], dtype=torch.float32))\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVHU71P5EWsm",
        "outputId": "63a0d0e9-830c-4ba9-ba0b-c0d7c795fba3"
      },
      "source": [
        "train_model(hidden_dim=20, lr=0.01, num_steps=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for hard examples=[tensor([-0.1775]), tensor([-0.1774])]\n",
            "Logits for hard examples=[tensor([0.0095]), tensor([0.0097])]\n",
            "Logits for hard examples=[tensor([0.0259]), tensor([0.0263])]\n",
            "Logits for hard examples=[tensor([-0.0017]), tensor([-0.0009])]\n",
            "Logits for hard examples=[tensor([-0.0034]), tensor([7.1838e-05])]\n",
            "Logits for hard examples=[tensor([-3.0632]), tensor([3.5531])]\n",
            "Logits for hard examples=[tensor([-11.0094]), tensor([10.8645])]\n",
            "Logits for hard examples=[tensor([-12.1567]), tensor([11.9191])]\n",
            "Logits for hard examples=[tensor([-12.3136]), tensor([12.0644])]\n",
            "Logits for hard examples=[tensor([-12.3368]), tensor([12.0874])]\n",
            "Logits for hard examples=[tensor([-12.3420]), tensor([12.0939])]\n",
            "Logits for hard examples=[tensor([-12.3448]), tensor([12.0982])]\n",
            "Logits for hard examples=[tensor([-12.3473]), tensor([12.1022])]\n",
            "Logits for hard examples=[tensor([-12.3498]), tensor([12.1062])]\n",
            "Logits for hard examples=[tensor([-12.3523]), tensor([12.1101])]\n",
            "Logits for hard examples=[tensor([-12.3547]), tensor([12.1140])]\n",
            "Logits for hard examples=[tensor([-12.3571]), tensor([12.1179])]\n",
            "Logits for hard examples=[tensor([-12.3595]), tensor([12.1218])]\n",
            "Logits for hard examples=[tensor([-12.3619]), tensor([12.1257])]\n",
            "Logits for hard examples=[tensor([-12.3643]), tensor([12.1295])]\n",
            "Logits for hard examples=[tensor([-12.3667]), tensor([12.1333])]\n",
            "Logits for hard examples=[tensor([-12.3690]), tensor([12.1372])]\n",
            "Logits for hard examples=[tensor([-12.3714]), tensor([12.1410])]\n",
            "Logits for hard examples=[tensor([-12.3737]), tensor([12.1448])]\n",
            "Logits for hard examples=[tensor([-12.3761]), tensor([12.1486])]\n",
            "Logits for hard examples=[tensor([-12.3784]), tensor([12.1523])]\n",
            "Logits for hard examples=[tensor([-12.3807]), tensor([12.1560])]\n",
            "Logits for hard examples=[tensor([-12.3831]), tensor([12.1597])]\n",
            "Logits for hard examples=[tensor([-12.3854]), tensor([12.1634])]\n",
            "Logits for hard examples=[tensor([-12.3877]), tensor([12.1670])]\n",
            "Logits for hard examples=[tensor([-12.3900]), tensor([12.1707])]\n",
            "Logits for hard examples=[tensor([-12.3923]), tensor([12.1743])]\n",
            "Logits for hard examples=[tensor([-12.3946]), tensor([12.1779])]\n",
            "Logits for hard examples=[tensor([-12.3969]), tensor([12.1814])]\n",
            "Logits for hard examples=[tensor([-12.3992]), tensor([12.1850])]\n",
            "Logits for hard examples=[tensor([-12.4015]), tensor([12.1886])]\n",
            "Logits for hard examples=[tensor([-12.4038]), tensor([12.1922])]\n",
            "Logits for hard examples=[tensor([-12.4061]), tensor([12.1957])]\n",
            "Logits for hard examples=[tensor([-12.4083]), tensor([12.1992])]\n",
            "Logits for hard examples=[tensor([-12.4106]), tensor([12.2027])]\n",
            "Logits for hard examples=[tensor([-12.4129]), tensor([12.2063])]\n",
            "Logits for hard examples=[tensor([-12.4152]), tensor([12.2098])]\n",
            "Logits for hard examples=[tensor([-12.4174]), tensor([12.2133])]\n",
            "Logits for hard examples=[tensor([-12.4197]), tensor([12.2168])]\n",
            "Logits for hard examples=[tensor([-12.4220]), tensor([12.2203])]\n",
            "Logits for hard examples=[tensor([-12.4242]), tensor([12.2237])]\n",
            "Logits for hard examples=[tensor([-12.4265]), tensor([12.2271])]\n",
            "Logits for hard examples=[tensor([-12.4287]), tensor([12.2306])]\n",
            "Logits for hard examples=[tensor([-12.4310]), tensor([12.2340])]\n",
            "Logits for hard examples=[tensor([-12.4332]), tensor([12.2374])]\n",
            "Logits for hard examples=[tensor([-12.4355]), tensor([12.2409])]\n",
            "Logits for hard examples=[tensor([-12.4377]), tensor([12.2443])]\n",
            "Logits for hard examples=[tensor([-12.4400]), tensor([12.2476])]\n",
            "Logits for hard examples=[tensor([-12.4422]), tensor([12.2510])]\n",
            "Logits for hard examples=[tensor([-12.4444]), tensor([12.2543])]\n",
            "Logits for hard examples=[tensor([-12.4467]), tensor([12.2577])]\n",
            "Logits for hard examples=[tensor([-12.4489]), tensor([12.2610])]\n",
            "Logits for hard examples=[tensor([-12.4511]), tensor([12.2644])]\n",
            "Logits for hard examples=[tensor([-12.4534]), tensor([12.2677])]\n",
            "Logits for hard examples=[tensor([-12.4556]), tensor([12.2710])]\n",
            "Logits for hard examples=[tensor([-12.4578]), tensor([12.2743])]\n",
            "Logits for hard examples=[tensor([-12.4600]), tensor([12.2776])]\n",
            "Logits for hard examples=[tensor([-12.4622]), tensor([12.2809])]\n",
            "Logits for hard examples=[tensor([-12.4644]), tensor([12.2841])]\n",
            "Logits for hard examples=[tensor([-12.4666]), tensor([12.2874])]\n",
            "Logits for hard examples=[tensor([-12.4688]), tensor([12.2907])]\n",
            "Logits for hard examples=[tensor([-12.4710]), tensor([12.2940])]\n",
            "Logits for hard examples=[tensor([-12.4731]), tensor([12.2972])]\n",
            "Logits for hard examples=[tensor([-12.4753]), tensor([12.3004])]\n",
            "Logits for hard examples=[tensor([-12.4775]), tensor([12.3036])]\n",
            "Logits for hard examples=[tensor([-12.4797]), tensor([12.3069])]\n",
            "Logits for hard examples=[tensor([-12.4819]), tensor([12.3101])]\n",
            "Logits for hard examples=[tensor([-12.4841]), tensor([12.3133])]\n",
            "Logits for hard examples=[tensor([-12.4862]), tensor([12.3165])]\n",
            "Logits for hard examples=[tensor([-12.4884]), tensor([12.3197])]\n",
            "Logits for hard examples=[tensor([-12.4906]), tensor([12.3228])]\n",
            "Logits for hard examples=[tensor([-12.4928]), tensor([12.3259])]\n",
            "Logits for hard examples=[tensor([-12.4950]), tensor([12.3290])]\n",
            "Logits for hard examples=[tensor([-12.4971]), tensor([12.3321])]\n",
            "Logits for hard examples=[tensor([-12.4993]), tensor([12.3352])]\n",
            "Logits for hard examples=[tensor([-12.5015]), tensor([12.3383])]\n",
            "Logits for hard examples=[tensor([-12.5036]), tensor([12.3415])]\n",
            "Logits for hard examples=[tensor([-12.5058]), tensor([12.3446])]\n",
            "Logits for hard examples=[tensor([-12.5079]), tensor([12.3477])]\n",
            "Logits for hard examples=[tensor([-12.5101]), tensor([12.3507])]\n",
            "Logits for hard examples=[tensor([-12.5123]), tensor([12.3537])]\n",
            "Logits for hard examples=[tensor([-12.5144]), tensor([12.3566])]\n",
            "Logits for hard examples=[tensor([-12.5166]), tensor([12.3596])]\n",
            "Logits for hard examples=[tensor([-12.5187]), tensor([12.3626])]\n",
            "Logits for hard examples=[tensor([-12.5209]), tensor([12.3656])]\n",
            "Logits for hard examples=[tensor([-12.5231]), tensor([12.3685])]\n",
            "Logits for hard examples=[tensor([-12.5252]), tensor([12.3715])]\n",
            "Logits for hard examples=[tensor([-12.5274]), tensor([12.3745])]\n",
            "Logits for hard examples=[tensor([-12.5295]), tensor([12.3773])]\n",
            "Logits for hard examples=[tensor([-12.5317]), tensor([12.3802])]\n",
            "Logits for hard examples=[tensor([-12.5338]), tensor([12.3831])]\n",
            "Logits for hard examples=[tensor([-12.5359]), tensor([12.3860])]\n",
            "Logits for hard examples=[tensor([-12.5381]), tensor([12.3889])]\n",
            "Logits for hard examples=[tensor([-12.5402]), tensor([12.3918])]\n",
            "Logits for hard examples=[tensor([-12.5423]), tensor([12.3946])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lt7dRkJDfqd"
      },
      "source": [
        "## To implement\n",
        "\n",
        "1. Check for what values of `SEQUENCE_LEN` the model is able to discriminate between the two hard examples (after training).\n",
        "\n",
        "Note that for steps 2-4 you may need to change the value of `num_steps`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 11\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model(hidden_dim=20, lr=0.01, num_steps=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlPh77b6F1pO",
        "outputId": "9ac78717-380f-41f8-f1a0-cbe0bcf5cf5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for hard examples=[tensor([0.0854]), tensor([0.0853])]\n",
            "Logits for hard examples=[tensor([0.0340]), tensor([0.0340])]\n",
            "Logits for hard examples=[tensor([-0.0007]), tensor([-0.0007])]\n",
            "Logits for hard examples=[tensor([-0.0031]), tensor([-0.0031])]\n",
            "Logits for hard examples=[tensor([0.0018]), tensor([0.0018])]\n",
            "Logits for hard examples=[tensor([0.0021]), tensor([0.0021])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0015]), tensor([0.0016])]\n",
            "Logits for hard examples=[tensor([0.0014]), tensor([0.0015])]\n",
            "Logits for hard examples=[tensor([0.0016]), tensor([0.0019])]\n",
            "Logits for hard examples=[tensor([0.0015]), tensor([0.0020])]\n",
            "Logits for hard examples=[tensor([0.0004]), tensor([0.0017])]\n",
            "Logits for hard examples=[tensor([-0.0101]), tensor([0.0129])]\n",
            "Logits for hard examples=[tensor([-9.9209]), tensor([10.1397])]\n",
            "Logits for hard examples=[tensor([-13.2050]), tensor([12.8995])]\n",
            "Logits for hard examples=[tensor([-13.6363]), tensor([13.2736])]\n",
            "Logits for hard examples=[tensor([-13.6943]), tensor([13.3251])]\n",
            "Logits for hard examples=[tensor([-13.7023]), tensor([13.3333])]\n",
            "Logits for hard examples=[tensor([-13.7037]), tensor([13.3357])]\n",
            "Logits for hard examples=[tensor([-13.7042]), tensor([13.3373])]\n",
            "Logits for hard examples=[tensor([-13.7046]), tensor([13.3388])]\n",
            "Logits for hard examples=[tensor([-13.7049]), tensor([13.3403])]\n",
            "Logits for hard examples=[tensor([-13.7052]), tensor([13.3417])]\n",
            "Logits for hard examples=[tensor([-13.7055]), tensor([13.3431])]\n",
            "Logits for hard examples=[tensor([-13.7058]), tensor([13.3445])]\n",
            "Logits for hard examples=[tensor([-13.7062]), tensor([13.3458])]\n",
            "Logits for hard examples=[tensor([-13.7065]), tensor([13.3472])]\n",
            "Logits for hard examples=[tensor([-13.7068]), tensor([13.3486])]\n",
            "Logits for hard examples=[tensor([-13.7071]), tensor([13.3500])]\n",
            "Logits for hard examples=[tensor([-13.7074]), tensor([13.3513])]\n",
            "Logits for hard examples=[tensor([-13.7078]), tensor([13.3527])]\n",
            "Logits for hard examples=[tensor([-13.7081]), tensor([13.3541])]\n",
            "Logits for hard examples=[tensor([-13.7084]), tensor([13.3555])]\n",
            "Logits for hard examples=[tensor([-13.7087]), tensor([13.3569])]\n",
            "Logits for hard examples=[tensor([-13.7091]), tensor([13.3582])]\n",
            "Logits for hard examples=[tensor([-13.7094]), tensor([13.3596])]\n",
            "Logits for hard examples=[tensor([-13.7097]), tensor([13.3610])]\n",
            "Logits for hard examples=[tensor([-13.7100]), tensor([13.3624])]\n",
            "Logits for hard examples=[tensor([-13.7103]), tensor([13.3638])]\n",
            "Logits for hard examples=[tensor([-13.7107]), tensor([13.3651])]\n",
            "Logits for hard examples=[tensor([-13.7110]), tensor([13.3665])]\n",
            "Logits for hard examples=[tensor([-13.7113]), tensor([13.3679])]\n",
            "Logits for hard examples=[tensor([-13.7116]), tensor([13.3693])]\n",
            "Logits for hard examples=[tensor([-13.7119]), tensor([13.3707])]\n",
            "Logits for hard examples=[tensor([-13.7123]), tensor([13.3720])]\n",
            "Logits for hard examples=[tensor([-13.7126]), tensor([13.3734])]\n",
            "Logits for hard examples=[tensor([-13.7129]), tensor([13.3748])]\n",
            "Logits for hard examples=[tensor([-13.7132]), tensor([13.3762])]\n",
            "Logits for hard examples=[tensor([-13.7136]), tensor([13.3776])]\n",
            "Logits for hard examples=[tensor([-13.7139]), tensor([13.3789])]\n",
            "Logits for hard examples=[tensor([-13.7142]), tensor([13.3803])]\n",
            "Logits for hard examples=[tensor([-13.7145]), tensor([13.3817])]\n",
            "Logits for hard examples=[tensor([-13.7148]), tensor([13.3831])]\n",
            "Logits for hard examples=[tensor([-13.7152]), tensor([13.3845])]\n",
            "Logits for hard examples=[tensor([-13.7155]), tensor([13.3858])]\n",
            "Logits for hard examples=[tensor([-13.7158]), tensor([13.3872])]\n",
            "Logits for hard examples=[tensor([-13.7161]), tensor([13.3886])]\n",
            "Logits for hard examples=[tensor([-13.7164]), tensor([13.3900])]\n",
            "Logits for hard examples=[tensor([-13.7168]), tensor([13.3914])]\n",
            "Logits for hard examples=[tensor([-13.7171]), tensor([13.3928])]\n",
            "Logits for hard examples=[tensor([-13.7174]), tensor([13.3941])]\n",
            "Logits for hard examples=[tensor([-13.7177]), tensor([13.3955])]\n",
            "Logits for hard examples=[tensor([-13.7181]), tensor([13.3969])]\n",
            "Logits for hard examples=[tensor([-13.7184]), tensor([13.3983])]\n",
            "Logits for hard examples=[tensor([-13.7187]), tensor([13.3997])]\n",
            "Logits for hard examples=[tensor([-13.7190]), tensor([13.4010])]\n",
            "Logits for hard examples=[tensor([-13.7193]), tensor([13.4024])]\n",
            "Logits for hard examples=[tensor([-13.7197]), tensor([13.4038])]\n",
            "Logits for hard examples=[tensor([-13.7200]), tensor([13.4052])]\n",
            "Logits for hard examples=[tensor([-13.7203]), tensor([13.4066])]\n",
            "Logits for hard examples=[tensor([-13.7206]), tensor([13.4080])]\n",
            "Logits for hard examples=[tensor([-13.7209]), tensor([13.4093])]\n",
            "Logits for hard examples=[tensor([-13.7213]), tensor([13.4107])]\n",
            "Logits for hard examples=[tensor([-13.7216]), tensor([13.4121])]\n",
            "Logits for hard examples=[tensor([-13.7219]), tensor([13.4135])]\n",
            "Logits for hard examples=[tensor([-13.7222]), tensor([13.4149])]\n",
            "Logits for hard examples=[tensor([-13.7225]), tensor([13.4163])]\n",
            "Logits for hard examples=[tensor([-13.7229]), tensor([13.4176])]\n",
            "Logits for hard examples=[tensor([-13.7232]), tensor([13.4190])]\n",
            "Logits for hard examples=[tensor([-13.7235]), tensor([13.4203])]\n",
            "Logits for hard examples=[tensor([-13.7238]), tensor([13.4216])]\n",
            "Logits for hard examples=[tensor([-13.7241]), tensor([13.4228])]\n",
            "Logits for hard examples=[tensor([-13.7245]), tensor([13.4241])]\n",
            "Logits for hard examples=[tensor([-13.7248]), tensor([13.4254])]\n",
            "Logits for hard examples=[tensor([-13.7251]), tensor([13.4267])]\n",
            "Logits for hard examples=[tensor([-13.7254]), tensor([13.4280])]\n",
            "Logits for hard examples=[tensor([-13.7258]), tensor([13.4293])]\n",
            "Logits for hard examples=[tensor([-13.7261]), tensor([13.4306])]\n",
            "Logits for hard examples=[tensor([-13.7264]), tensor([13.4319])]\n",
            "Logits for hard examples=[tensor([-13.7267]), tensor([13.4332])]\n",
            "Logits for hard examples=[tensor([-13.7270]), tensor([13.4345])]\n",
            "Logits for hard examples=[tensor([-13.7274]), tensor([13.4358])]\n",
            "Logits for hard examples=[tensor([-13.7277]), tensor([13.4371])]\n",
            "Logits for hard examples=[tensor([-13.7280]), tensor([13.4384])]\n",
            "Logits for hard examples=[tensor([-13.7283]), tensor([13.4397])]\n",
            "Logits for hard examples=[tensor([-13.7286]), tensor([13.4410])]\n",
            "Logits for hard examples=[tensor([-13.7290]), tensor([13.4422])]\n",
            "Logits for hard examples=[tensor([-13.7293]), tensor([13.4435])]\n",
            "Logits for hard examples=[tensor([-13.7296]), tensor([13.4448])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 12\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model(hidden_dim=20, lr=0.01, num_steps=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0a9dfc-f10a-4106-e838-31034f4d7359",
        "id": "HdDixnTaGXxR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for hard examples=[tensor([-0.0323]), tensor([-0.0323])]\n",
            "Logits for hard examples=[tensor([0.0250]), tensor([0.0249])]\n",
            "Logits for hard examples=[tensor([-0.0012]), tensor([-0.0012])]\n",
            "Logits for hard examples=[tensor([-0.0006]), tensor([-0.0006])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0011]), tensor([0.0011])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([0.0014]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([0.0015]), tensor([0.0015])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([1.7098e-05]), tensor([8.5090e-05])]\n",
            "Logits for hard examples=[tensor([0.0016]), tensor([0.0017])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([0.0012]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([0.0041]), tensor([0.0044])]\n",
            "Logits for hard examples=[tensor([0.0010]), tensor([0.0016])]\n",
            "Logits for hard examples=[tensor([8.6417e-06]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0366]), tensor([0.0376])]\n",
            "Logits for hard examples=[tensor([-0.0093]), tensor([-0.0077])]\n",
            "Logits for hard examples=[tensor([-0.1266]), tensor([0.0952])]\n",
            "Logits for hard examples=[tensor([-0.4075]), tensor([-0.4076])]\n",
            "Logits for hard examples=[tensor([0.1384]), tensor([0.1384])]\n",
            "Logits for hard examples=[tensor([-0.0509]), tensor([-0.0509])]\n",
            "Logits for hard examples=[tensor([-0.0006]), tensor([-0.0006])]\n",
            "Logits for hard examples=[tensor([0.0103]), tensor([0.0103])]\n",
            "Logits for hard examples=[tensor([0.0016]), tensor([0.0016])]\n",
            "Logits for hard examples=[tensor([0.0015]), tensor([0.0015])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0027])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0024])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0024])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0022]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.1219]), tensor([0.1220])]\n",
            "Logits for hard examples=[tensor([0.0349]), tensor([0.0350])]\n",
            "Logits for hard examples=[tensor([-0.0151]), tensor([-0.0150])]\n",
            "Logits for hard examples=[tensor([-0.0014]), tensor([-0.0013])]\n",
            "Logits for hard examples=[tensor([0.0048]), tensor([0.0049])]\n",
            "Logits for hard examples=[tensor([0.0028]), tensor([0.0029])]\n",
            "Logits for hard examples=[tensor([0.0020]), tensor([0.0021])]\n",
            "Logits for hard examples=[tensor([0.0023]), tensor([0.0024])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0023]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0023]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0023]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0023]), tensor([0.0027])]\n",
            "Logits for hard examples=[tensor([0.0022]), tensor([0.0029])]\n",
            "Logits for hard examples=[tensor([0.0019]), tensor([0.0040])]\n",
            "Logits for hard examples=[tensor([-0.0920]), tensor([0.1087])]\n",
            "Logits for hard examples=[tensor([-3.8695]), tensor([4.5852])]\n",
            "Logits for hard examples=[tensor([-5.3979]), tensor([5.7348])]\n",
            "Logits for hard examples=[tensor([-5.8835]), tensor([6.0579])]\n",
            "Logits for hard examples=[tensor([-6.1379]), tensor([6.2899])]\n",
            "Logits for hard examples=[tensor([-6.3522]), tensor([6.5015])]\n",
            "Logits for hard examples=[tensor([-6.5551]), tensor([6.7048])]\n",
            "Logits for hard examples=[tensor([-6.7469]), tensor([6.9002])]\n",
            "Logits for hard examples=[tensor([-6.9250]), tensor([7.0826])]\n",
            "Logits for hard examples=[tensor([-7.0874]), tensor([7.2482])]\n",
            "Logits for hard examples=[tensor([-7.2340]), tensor([7.3960])]\n",
            "Logits for hard examples=[tensor([-7.3660]), tensor([7.5269])]\n",
            "Logits for hard examples=[tensor([-7.4852]), tensor([7.6430])]\n",
            "Logits for hard examples=[tensor([-7.5932]), tensor([7.7465])]\n",
            "Logits for hard examples=[tensor([-7.6916]), tensor([7.8396])]\n",
            "Logits for hard examples=[tensor([-7.7819]), tensor([7.9240])]\n",
            "Logits for hard examples=[tensor([-7.8651]), tensor([8.0011])]\n",
            "Logits for hard examples=[tensor([-7.9423]), tensor([8.0721])]\n",
            "Logits for hard examples=[tensor([-8.0142]), tensor([8.1379])]\n",
            "Logits for hard examples=[tensor([-8.0816]), tensor([8.1992])]\n",
            "Logits for hard examples=[tensor([-8.1448]), tensor([8.2566])]\n",
            "Logits for hard examples=[tensor([-8.2045]), tensor([8.3106])]\n",
            "Logits for hard examples=[tensor([-8.2609]), tensor([8.3616])]\n",
            "Logits for hard examples=[tensor([-8.3145]), tensor([8.4100])]\n",
            "Logits for hard examples=[tensor([-8.3655]), tensor([8.4560])]\n",
            "Logits for hard examples=[tensor([-8.4142]), tensor([8.4998])]\n",
            "Logits for hard examples=[tensor([-8.4608]), tensor([8.5416])]\n",
            "Logits for hard examples=[tensor([-8.5054]), tensor([8.5817])]\n",
            "Logits for hard examples=[tensor([-8.5482]), tensor([8.6202])]\n",
            "Logits for hard examples=[tensor([-8.5894]), tensor([8.6572])]\n",
            "Logits for hard examples=[tensor([-8.6290]), tensor([8.6928])]\n",
            "Logits for hard examples=[tensor([-8.6673]), tensor([8.7272])]\n",
            "Logits for hard examples=[tensor([-8.7043]), tensor([8.7604])]\n",
            "Logits for hard examples=[tensor([-8.7400]), tensor([8.7925])]\n",
            "Logits for hard examples=[tensor([-8.7746]), tensor([8.8236])]\n",
            "Logits for hard examples=[tensor([-8.8082]), tensor([8.8537])]\n",
            "Logits for hard examples=[tensor([-8.8408]), tensor([8.8829])]\n",
            "Logits for hard examples=[tensor([-8.8724]), tensor([8.9112])]\n",
            "Logits for hard examples=[tensor([-8.9032]), tensor([8.9388])]\n",
            "Logits for hard examples=[tensor([-8.9331]), tensor([8.9657])]\n",
            "Logits for hard examples=[tensor([-8.9622]), tensor([8.9918])]\n",
            "Logits for hard examples=[tensor([-8.9906]), tensor([9.0172])]\n",
            "Logits for hard examples=[tensor([-9.0184]), tensor([9.0420])]\n",
            "Logits for hard examples=[tensor([-9.0454]), tensor([9.0662])]\n",
            "Logits for hard examples=[tensor([-9.0718]), tensor([9.0898])]\n",
            "Logits for hard examples=[tensor([-9.0976]), tensor([9.1129])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 13\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model(hidden_dim=20, lr=0.01, num_steps=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1fc9d77-ba8e-4c1f-daaf-0fabb12663e5",
        "id": "MnjSfnl9GbWJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for hard examples=[tensor([-0.1013]), tensor([-0.1013])]\n",
            "Logits for hard examples=[tensor([-0.0147]), tensor([-0.0147])]\n",
            "Logits for hard examples=[tensor([0.0120]), tensor([0.0121])]\n",
            "Logits for hard examples=[tensor([0.0062]), tensor([0.0064])]\n",
            "Logits for hard examples=[tensor([0.0001]), tensor([0.0005])]\n",
            "Logits for hard examples=[tensor([0.0005]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([-7.7274e-05]), tensor([0.0053])]\n",
            "Logits for hard examples=[tensor([-8.3054]), tensor([7.3554])]\n",
            "Logits for hard examples=[tensor([-14.7091]), tensor([12.6479])]\n",
            "Logits for hard examples=[tensor([-15.5905]), tensor([13.3639])]\n",
            "Logits for hard examples=[tensor([-15.7088]), tensor([13.4607])]\n",
            "Logits for hard examples=[tensor([-15.7247]), tensor([13.4746])]\n",
            "Logits for hard examples=[tensor([-15.7268]), tensor([13.4774])]\n",
            "Logits for hard examples=[tensor([-15.7271]), tensor([13.4787])]\n",
            "Logits for hard examples=[tensor([-15.7271]), tensor([13.4798])]\n",
            "Logits for hard examples=[tensor([-15.7270]), tensor([13.4809])]\n",
            "Logits for hard examples=[tensor([-15.7270]), tensor([13.4819])]\n",
            "Logits for hard examples=[tensor([-15.7269]), tensor([13.4830])]\n",
            "Logits for hard examples=[tensor([-15.7268]), tensor([13.4840])]\n",
            "Logits for hard examples=[tensor([-15.7268]), tensor([13.4851])]\n",
            "Logits for hard examples=[tensor([-15.7267]), tensor([13.4861])]\n",
            "Logits for hard examples=[tensor([-15.7267]), tensor([13.4872])]\n",
            "Logits for hard examples=[tensor([-15.7266]), tensor([13.4882])]\n",
            "Logits for hard examples=[tensor([-15.7265]), tensor([13.4893])]\n",
            "Logits for hard examples=[tensor([-15.7265]), tensor([13.4903])]\n",
            "Logits for hard examples=[tensor([-15.7264]), tensor([13.4914])]\n",
            "Logits for hard examples=[tensor([-15.7263]), tensor([13.4924])]\n",
            "Logits for hard examples=[tensor([-15.7263]), tensor([13.4934])]\n",
            "Logits for hard examples=[tensor([-15.7262]), tensor([13.4945])]\n",
            "Logits for hard examples=[tensor([-15.7262]), tensor([13.4955])]\n",
            "Logits for hard examples=[tensor([-15.7261]), tensor([13.4966])]\n",
            "Logits for hard examples=[tensor([-15.7260]), tensor([13.4976])]\n",
            "Logits for hard examples=[tensor([-15.7260]), tensor([13.4987])]\n",
            "Logits for hard examples=[tensor([-15.7259]), tensor([13.4997])]\n",
            "Logits for hard examples=[tensor([-15.7259]), tensor([13.5007])]\n",
            "Logits for hard examples=[tensor([-15.7258]), tensor([13.5017])]\n",
            "Logits for hard examples=[tensor([-15.7258]), tensor([13.5027])]\n",
            "Logits for hard examples=[tensor([-15.7257]), tensor([13.5036])]\n",
            "Logits for hard examples=[tensor([-15.7257]), tensor([13.5046])]\n",
            "Logits for hard examples=[tensor([-15.7256]), tensor([13.5055])]\n",
            "Logits for hard examples=[tensor([-15.7256]), tensor([13.5065])]\n",
            "Logits for hard examples=[tensor([-15.7255]), tensor([13.5075])]\n",
            "Logits for hard examples=[tensor([-15.7255]), tensor([13.5084])]\n",
            "Logits for hard examples=[tensor([-15.7254]), tensor([13.5094])]\n",
            "Logits for hard examples=[tensor([-15.7254]), tensor([13.5104])]\n",
            "Logits for hard examples=[tensor([-15.7253]), tensor([13.5113])]\n",
            "Logits for hard examples=[tensor([-15.7253]), tensor([13.5123])]\n",
            "Logits for hard examples=[tensor([-15.7252]), tensor([13.5132])]\n",
            "Logits for hard examples=[tensor([-15.7252]), tensor([13.5142])]\n",
            "Logits for hard examples=[tensor([-15.7251]), tensor([13.5152])]\n",
            "Logits for hard examples=[tensor([-15.7251]), tensor([13.5161])]\n",
            "Logits for hard examples=[tensor([-15.7250]), tensor([13.5171])]\n",
            "Logits for hard examples=[tensor([-15.7250]), tensor([13.5181])]\n",
            "Logits for hard examples=[tensor([-15.7249]), tensor([13.5190])]\n",
            "Logits for hard examples=[tensor([-15.7249]), tensor([13.5200])]\n",
            "Logits for hard examples=[tensor([-15.7248]), tensor([13.5209])]\n",
            "Logits for hard examples=[tensor([-15.7248]), tensor([13.5219])]\n",
            "Logits for hard examples=[tensor([-15.7247]), tensor([13.5229])]\n",
            "Logits for hard examples=[tensor([-15.7247]), tensor([13.5238])]\n",
            "Logits for hard examples=[tensor([-15.7247]), tensor([13.5248])]\n",
            "Logits for hard examples=[tensor([-15.7246]), tensor([13.5258])]\n",
            "Logits for hard examples=[tensor([-15.7246]), tensor([13.5267])]\n",
            "Logits for hard examples=[tensor([-15.7245]), tensor([13.5277])]\n",
            "Logits for hard examples=[tensor([-15.7245]), tensor([13.5286])]\n",
            "Logits for hard examples=[tensor([-15.7244]), tensor([13.5296])]\n",
            "Logits for hard examples=[tensor([-15.7244]), tensor([13.5306])]\n",
            "Logits for hard examples=[tensor([-15.7243]), tensor([13.5315])]\n",
            "Logits for hard examples=[tensor([-15.7243]), tensor([13.5325])]\n",
            "Logits for hard examples=[tensor([-15.7242]), tensor([13.5335])]\n",
            "Logits for hard examples=[tensor([-15.7242]), tensor([13.5344])]\n",
            "Logits for hard examples=[tensor([-15.7241]), tensor([13.5354])]\n",
            "Logits for hard examples=[tensor([-15.7241]), tensor([13.5363])]\n",
            "Logits for hard examples=[tensor([-15.7240]), tensor([13.5373])]\n",
            "Logits for hard examples=[tensor([-15.7240]), tensor([13.5383])]\n",
            "Logits for hard examples=[tensor([-15.7239]), tensor([13.5392])]\n",
            "Logits for hard examples=[tensor([-15.7239]), tensor([13.5402])]\n",
            "Logits for hard examples=[tensor([-15.7238]), tensor([13.5411])]\n",
            "Logits for hard examples=[tensor([-15.7238]), tensor([13.5421])]\n",
            "Logits for hard examples=[tensor([-15.7237]), tensor([13.5431])]\n",
            "Logits for hard examples=[tensor([-15.7237]), tensor([13.5440])]\n",
            "Logits for hard examples=[tensor([-15.7237]), tensor([13.5450])]\n",
            "Logits for hard examples=[tensor([-15.7236]), tensor([13.5459])]\n",
            "Logits for hard examples=[tensor([-15.7236]), tensor([13.5469])]\n",
            "Logits for hard examples=[tensor([-15.7235]), tensor([13.5479])]\n",
            "Logits for hard examples=[tensor([-15.7235]), tensor([13.5488])]\n",
            "Logits for hard examples=[tensor([-15.7234]), tensor([13.5498])]\n",
            "Logits for hard examples=[tensor([-15.7234]), tensor([13.5507])]\n",
            "Logits for hard examples=[tensor([-15.7233]), tensor([13.5517])]\n",
            "Logits for hard examples=[tensor([-15.7233]), tensor([13.5527])]\n",
            "Logits for hard examples=[tensor([-15.7232]), tensor([13.5536])]\n",
            "Logits for hard examples=[tensor([-15.7232]), tensor([13.5546])]\n",
            "Logits for hard examples=[tensor([-15.7231]), tensor([13.5555])]\n",
            "Logits for hard examples=[tensor([-15.7231]), tensor([13.5565])]\n",
            "Logits for hard examples=[tensor([-15.7230]), tensor([13.5575])]\n",
            "Logits for hard examples=[tensor([-15.7230]), tensor([13.5584])]\n",
            "Logits for hard examples=[tensor([-15.7229]), tensor([13.5594])]\n",
            "Logits for hard examples=[tensor([-15.7229]), tensor([13.5603])]\n",
            "Logits for hard examples=[tensor([-15.7228]), tensor([13.5613])]\n",
            "Logits for hard examples=[tensor([-15.7228]), tensor([13.5623])]\n",
            "Logits for hard examples=[tensor([-15.7227]), tensor([13.5632])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 14\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model(hidden_dim=20, lr=0.01, num_steps=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93RZhbXul2WC",
        "outputId": "ad053dd6-d6bc-4695-c21d-8b38399c9050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for hard examples=[tensor([-0.0441]), tensor([-0.0441])]\n",
            "Logits for hard examples=[tensor([0.0232]), tensor([0.0232])]\n",
            "Logits for hard examples=[tensor([0.0038]), tensor([0.0038])]\n",
            "Logits for hard examples=[tensor([-0.0020]), tensor([-0.0020])]\n",
            "Logits for hard examples=[tensor([0.0023]), tensor([0.0023])]\n",
            "Logits for hard examples=[tensor([0.0012]), tensor([0.0012])]\n",
            "Logits for hard examples=[tensor([0.0012]), tensor([0.0012])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0012]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0012]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0012]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0012]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([0.0012]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0015])]\n",
            "Logits for hard examples=[tensor([0.0008]), tensor([0.0011])]\n",
            "Logits for hard examples=[tensor([0.0016]), tensor([0.0024])]\n",
            "Logits for hard examples=[tensor([-0.0008]), tensor([0.0137])]\n",
            "Logits for hard examples=[tensor([-0.1400]), tensor([-0.1400])]\n",
            "Logits for hard examples=[tensor([0.0491]), tensor([0.0491])]\n",
            "Logits for hard examples=[tensor([0.0174]), tensor([0.0174])]\n",
            "Logits for hard examples=[tensor([-0.0054]), tensor([-0.0054])]\n",
            "Logits for hard examples=[tensor([0.0051]), tensor([0.0051])]\n",
            "Logits for hard examples=[tensor([0.0037]), tensor([0.0037])]\n",
            "Logits for hard examples=[tensor([0.0030]), tensor([0.0030])]\n",
            "Logits for hard examples=[tensor([0.0034]), tensor([0.0034])]\n",
            "Logits for hard examples=[tensor([0.0031]), tensor([0.0031])]\n",
            "Logits for hard examples=[tensor([0.0032]), tensor([0.0032])]\n",
            "Logits for hard examples=[tensor([0.0031]), tensor([0.0031])]\n",
            "Logits for hard examples=[tensor([0.0030]), tensor([0.0030])]\n",
            "Logits for hard examples=[tensor([0.0030]), tensor([0.0030])]\n",
            "Logits for hard examples=[tensor([0.0029]), tensor([0.0029])]\n",
            "Logits for hard examples=[tensor([0.0029]), tensor([0.0029])]\n",
            "Logits for hard examples=[tensor([0.0029]), tensor([0.0029])]\n",
            "Logits for hard examples=[tensor([0.0028]), tensor([0.0028])]\n",
            "Logits for hard examples=[tensor([0.0028]), tensor([0.0028])]\n",
            "Logits for hard examples=[tensor([0.0028]), tensor([0.0028])]\n",
            "Logits for hard examples=[tensor([0.0028]), tensor([0.0028])]\n",
            "Logits for hard examples=[tensor([0.0027]), tensor([0.0027])]\n",
            "Logits for hard examples=[tensor([0.0027]), tensor([0.0027])]\n",
            "Logits for hard examples=[tensor([0.0027]), tensor([0.0027])]\n",
            "Logits for hard examples=[tensor([0.0027]), tensor([0.0027])]\n",
            "Logits for hard examples=[tensor([0.0027]), tensor([0.0027])]\n",
            "Logits for hard examples=[tensor([0.0027]), tensor([0.0027])]\n",
            "Logits for hard examples=[tensor([0.0027]), tensor([0.0027])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0026]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0026])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0025]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0024]), tensor([0.0025])]\n",
            "Logits for hard examples=[tensor([0.0023]), tensor([0.0024])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 15\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model(hidden_dim=20, lr=0.01, num_steps=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fF3AJFZk_1_",
        "outputId": "eeb2171b-d29e-4971-bee5-1cdc452cf58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for hard examples=[tensor([-0.1748]), tensor([-0.1748])]\n",
            "Logits for hard examples=[tensor([0.0550]), tensor([0.0550])]\n",
            "Logits for hard examples=[tensor([-0.0008]), tensor([-0.0008])]\n",
            "Logits for hard examples=[tensor([-0.0043]), tensor([-0.0043])]\n",
            "Logits for hard examples=[tensor([0.0046]), tensor([0.0046])]\n",
            "Logits for hard examples=[tensor([0.0002]), tensor([0.0002])]\n",
            "Logits for hard examples=[tensor([0.0014]), tensor([0.0014])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0012]), tensor([0.0012])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n",
            "Logits for hard examples=[tensor([0.0013]), tensor([0.0013])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is able to discriminate between the two hard examples for sequences of length smaller or equal to 13 and fails to do so on loges sequences."
      ],
      "metadata": {
        "id": "R29tGtIYoIvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Instead of training on `HARD_EXAMPLES` only, modify the training loop to train on sequences where zero may be in any position of the sequence (so any valid sequence of `Type 0`, not just the hardest one). After modifying the training loop check for what values of `SEQUENCE_LEN` you can train the model successfully."
      ],
      "metadata": {
        "id": "AyooRCQklaOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "\n",
        "SEQUENCE_LEN = 10\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "# Returns a sequence of ones or ones with a zero on a random index based on given label.\n",
        "def get_normal_sequence(label):\n",
        "    sequence = SEQUENCE_LEN*[1.]\n",
        "    if (label == 0):\n",
        "        sequence[random.randint(0, SEQUENCE_LEN-1)] = 0.\n",
        "    return sequence\n",
        "\n",
        "def eval_on_normal_examples(model):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "        for _, label in HARD_EXAMPLES:\n",
        "            sequence = get_normal_sequence(label)\n",
        "            input = torch.tensor(sequence).view(-1, 1, 1)\n",
        "            logit = model(input)\n",
        "            logits.append(logit.detach())\n",
        "        print(f'Logits for normal examples={logits}')\n",
        "\n",
        "\n",
        "def train_model_on_normal_examples(hidden_dim, lr, num_steps=10000):\n",
        "    model = Model(hidden_dim=hidden_dim)\n",
        "    loss_function = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "\n",
        "    for step in range(num_steps):  \n",
        "        if step % 100 == 0:\n",
        "            eval_on_normal_examples(model)\n",
        "\n",
        "        for _, label in HARD_EXAMPLES:\n",
        "            sequence = get_normal_sequence(label)\n",
        "            model.zero_grad()\n",
        "            logit = model(torch.tensor(sequence).view(-1, 1, 1))  \n",
        "            \n",
        "            loss = loss_function(logit.view(-1), torch.tensor([label], dtype=torch.float32))\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()   "
      ],
      "metadata": {
        "id": "PQa7FuSclY59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 10\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model_on_normal_examples(hidden_dim=20, lr=0.01, num_steps=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6rFT7enn8NJ",
        "outputId": "24bc157b-5c28-45fa-a55a-3b65c9d686a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for normal examples=[tensor([0.3403]), tensor([0.3471])]\n",
            "Logits for normal examples=[tensor([-0.8592]), tensor([0.3931])]\n",
            "Logits for normal examples=[tensor([0.0774]), tensor([0.0778])]\n",
            "Logits for normal examples=[tensor([-0.1452]), tensor([0.2099])]\n",
            "Logits for normal examples=[tensor([1.4544]), tensor([1.4544])]\n",
            "Logits for normal examples=[tensor([0.2659]), tensor([0.2659])]\n",
            "Logits for normal examples=[tensor([-0.9263]), tensor([-0.2137])]\n",
            "Logits for normal examples=[tensor([0.2077]), tensor([0.1392])]\n",
            "Logits for normal examples=[tensor([0.1637]), tensor([0.1637])]\n",
            "Logits for normal examples=[tensor([0.2536]), tensor([0.2536])]\n",
            "Logits for normal examples=[tensor([0.0455]), tensor([0.0455])]\n",
            "Logits for normal examples=[tensor([-0.0239]), tensor([-0.0240])]\n",
            "Logits for normal examples=[tensor([0.0174]), tensor([0.0174])]\n",
            "Logits for normal examples=[tensor([0.0183]), tensor([0.0183])]\n",
            "Logits for normal examples=[tensor([0.0125]), tensor([0.0125])]\n",
            "Logits for normal examples=[tensor([0.0136]), tensor([0.0136])]\n",
            "Logits for normal examples=[tensor([0.0143]), tensor([0.0142])]\n",
            "Logits for normal examples=[tensor([0.0139]), tensor([0.0139])]\n",
            "Logits for normal examples=[tensor([0.0139]), tensor([0.0139])]\n",
            "Logits for normal examples=[tensor([0.0139]), tensor([0.0139])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 20\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model_on_normal_examples(hidden_dim=20, lr=0.01, num_steps=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYAQE0skLWRf",
        "outputId": "b72888b6-69d8-45b6-bb65-7e6088b5e7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for normal examples=[tensor([-0.1796]), tensor([-0.1797])]\n",
            "Logits for normal examples=[tensor([0.0294]), tensor([0.0294])]\n",
            "Logits for normal examples=[tensor([-0.0512]), tensor([-0.0189])]\n",
            "Logits for normal examples=[tensor([0.0871]), tensor([0.0872])]\n",
            "Logits for normal examples=[tensor([0.1617]), tensor([0.1619])]\n",
            "Logits for normal examples=[tensor([0.0730]), tensor([0.0752])]\n",
            "Logits for normal examples=[tensor([0.3456]), tensor([0.3465])]\n",
            "Logits for normal examples=[tensor([0.0022]), tensor([0.0077])]\n",
            "Logits for normal examples=[tensor([-0.0541]), tensor([-0.0541])]\n",
            "Logits for normal examples=[tensor([-0.0686]), tensor([-0.0550])]\n",
            "Logits for normal examples=[tensor([-0.0969]), tensor([0.0520])]\n",
            "Logits for normal examples=[tensor([0.0629]), tensor([0.0621])]\n",
            "Logits for normal examples=[tensor([0.1110]), tensor([0.1111])]\n",
            "Logits for normal examples=[tensor([-0.0666]), tensor([-0.0665])]\n",
            "Logits for normal examples=[tensor([0.0287]), tensor([0.0289])]\n",
            "Logits for normal examples=[tensor([-0.0038]), tensor([-0.0031])]\n",
            "Logits for normal examples=[tensor([-0.0021]), tensor([0.0023])]\n",
            "Logits for normal examples=[tensor([-0.1241]), tensor([0.1436])]\n",
            "Logits for normal examples=[tensor([-0.4057]), tensor([0.5750])]\n",
            "Logits for normal examples=[tensor([-6.8446]), tensor([3.6014])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 25\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model_on_normal_examples(hidden_dim=20, lr=0.01, num_steps=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nLI-5LKLfEA",
        "outputId": "d1efb322-d4ff-4d13-a082-fb5acbddc70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for normal examples=[tensor([0.1276]), tensor([0.1277])]\n",
            "Logits for normal examples=[tensor([0.0346]), tensor([0.0360])]\n",
            "Logits for normal examples=[tensor([-0.0794]), tensor([-0.0718])]\n",
            "Logits for normal examples=[tensor([-0.0179]), tensor([-0.0168])]\n",
            "Logits for normal examples=[tensor([-0.0425]), tensor([-0.0193])]\n",
            "Logits for normal examples=[tensor([0.0101]), tensor([0.0102])]\n",
            "Logits for normal examples=[tensor([0.1903]), tensor([0.1955])]\n",
            "Logits for normal examples=[tensor([-0.0935]), tensor([-0.0935])]\n",
            "Logits for normal examples=[tensor([-0.5117]), tensor([-0.5116])]\n",
            "Logits for normal examples=[tensor([0.1132]), tensor([0.1136])]\n",
            "Logits for normal examples=[tensor([-0.2766]), tensor([-0.2756])]\n",
            "Logits for normal examples=[tensor([-0.0781]), tensor([-0.0083])]\n",
            "Logits for normal examples=[tensor([-0.2832]), tensor([-0.2831])]\n",
            "Logits for normal examples=[tensor([-0.0745]), tensor([-0.0745])]\n",
            "Logits for normal examples=[tensor([0.1123]), tensor([0.1130])]\n",
            "Logits for normal examples=[tensor([0.1075]), tensor([0.1079])]\n",
            "Logits for normal examples=[tensor([-0.0031]), tensor([-0.0031])]\n",
            "Logits for normal examples=[tensor([0.4825]), tensor([0.4722])]\n",
            "Logits for normal examples=[tensor([-0.1525]), tensor([-0.1453])]\n",
            "Logits for normal examples=[tensor([-3.0165]), tensor([0.4356])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 26\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model_on_normal_examples(hidden_dim=20, lr=0.01, num_steps=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GERTIX2sLs2P",
        "outputId": "049c6754-6986-4b0b-a646-02d2458ebc5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for normal examples=[tensor([-0.2013]), tensor([-0.2013])]\n",
            "Logits for normal examples=[tensor([0.0386]), tensor([0.0399])]\n",
            "Logits for normal examples=[tensor([-0.0189]), tensor([0.0342])]\n",
            "Logits for normal examples=[tensor([0.0349]), tensor([0.0351])]\n",
            "Logits for normal examples=[tensor([-0.1620]), tensor([-0.1398])]\n",
            "Logits for normal examples=[tensor([0.0205]), tensor([0.0211])]\n",
            "Logits for normal examples=[tensor([-0.5050]), tensor([-0.5050])]\n",
            "Logits for normal examples=[tensor([-0.1914]), tensor([-0.1914])]\n",
            "Logits for normal examples=[tensor([-0.0689]), tensor([-0.0674])]\n",
            "Logits for normal examples=[tensor([-0.0098]), tensor([-0.0098])]\n",
            "Logits for normal examples=[tensor([0.0893]), tensor([0.0898])]\n",
            "Logits for normal examples=[tensor([0.2192]), tensor([0.2198])]\n",
            "Logits for normal examples=[tensor([-0.1116]), tensor([0.1624])]\n",
            "Logits for normal examples=[tensor([0.2898]), tensor([0.2901])]\n",
            "Logits for normal examples=[tensor([-0.0883]), tensor([-0.0879])]\n",
            "Logits for normal examples=[tensor([0.2121]), tensor([0.2121])]\n",
            "Logits for normal examples=[tensor([-0.1034]), tensor([-0.1035])]\n",
            "Logits for normal examples=[tensor([0.0370]), tensor([0.0370])]\n",
            "Logits for normal examples=[tensor([-0.0072]), tensor([-0.0072])]\n",
            "Logits for normal examples=[tensor([0.0023]), tensor([0.0010])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When trained on normal sentences, the model is able to discriminate between two hard examples for sequences of length <= 25 (for num_steps=2000)."
      ],
      "metadata": {
        "id": "Wys4dSulqatN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Replace LSTM by a classic RNN and check for what values of `SEQUENCE_LEN` you can train the model successfully."
      ],
      "metadata": {
        "id": "a3hUXD5elmqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.RNN(1, self.hidden_dim)\n",
        "        self.hidden2label = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        sequence_len = x.shape[0]\n",
        "        logits = self.hidden2label(F.relu(out[-1].view(-1)))\n",
        "        return logits"
      ],
      "metadata": {
        "id": "lakwi_Xzq9M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "\n",
        "SEQUENCE_LEN = 10\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "def eval_on_normal_examples(model):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "        for _, label in HARD_EXAMPLES:\n",
        "            sequence = get_normal_sequence(label)\n",
        "            input = torch.tensor(sequence).view(-1, 1, 1)\n",
        "            logit = model(input)\n",
        "            logits.append(logit.detach())\n",
        "        print(f'Logits for normal examples={logits}')\n",
        "\n",
        "\n",
        "def train_model_on_normal_examples(hidden_dim, lr, num_steps=10000):\n",
        "    model = Model(hidden_dim=hidden_dim)\n",
        "    loss_function = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "\n",
        "    for step in range(num_steps):  \n",
        "        if step % 100 == 0:\n",
        "            eval_on_normal_examples(model)\n",
        "\n",
        "        for _, label in HARD_EXAMPLES:\n",
        "            sequence = get_normal_sequence(label)\n",
        "            model.zero_grad()\n",
        "            logit = model(torch.tensor(sequence).view(-1, 1, 1))  \n",
        "            \n",
        "            loss = loss_function(logit.view(-1), torch.tensor([label], dtype=torch.float32))\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()   \n",
        "\n",
        "def eval_on_hard_examples(model):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "        for sequence in HARD_EXAMPLES:\n",
        "            input = torch.tensor(sequence[0]).view(-1, 1, 1)\n",
        "            logit = model(input)\n",
        "            logits.append(logit.detach())\n",
        "        print(f'Logits for hard examples={logits}')\n",
        "\n",
        "\n",
        "def train_model_on_hard_examples(hidden_dim, lr, num_steps=10000):\n",
        "    model = Model(hidden_dim=hidden_dim)\n",
        "    loss_function = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "\n",
        "    for step in range(num_steps):  \n",
        "        if step % 100 == 0:\n",
        "            eval_on_hard_examples(model)\n",
        "\n",
        "        for sequence, label in HARD_EXAMPLES:\n",
        "            model.zero_grad()\n",
        "            logit = model(torch.tensor(sequence).view(-1, 1, 1))  \n",
        "            \n",
        "            loss = loss_function(logit.view(-1), torch.tensor([label], dtype=torch.float32))\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()   "
      ],
      "metadata": {
        "id": "lH9TgCHIopwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 5\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model_on_normal_examples(hidden_dim=20, lr=0.01, num_steps=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOcR60N9MLbJ",
        "outputId": "d8c0c606-a5f8-4479-c7d6-5798015e6e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for normal examples=[tensor([0.0534]), tensor([0.0551])]\n",
            "Logits for normal examples=[tensor([0.2178]), tensor([-0.0652])]\n",
            "Logits for normal examples=[tensor([-4.6292]), tensor([3.8117])]\n",
            "Logits for normal examples=[tensor([-8.3657]), tensor([6.0672])]\n",
            "Logits for normal examples=[tensor([-7.2535]), tensor([7.6421])]\n",
            "Logits for normal examples=[tensor([-8.4040]), tensor([7.7039])]\n",
            "Logits for normal examples=[tensor([-17.1719]), tensor([7.8308])]\n",
            "Logits for normal examples=[tensor([-9.5871]), tensor([7.9181])]\n",
            "Logits for normal examples=[tensor([-17.2917]), tensor([8.0080])]\n",
            "Logits for normal examples=[tensor([-10.0237]), tensor([8.0818])]\n",
            "Logits for normal examples=[tensor([-14.3475]), tensor([8.1574])]\n",
            "Logits for normal examples=[tensor([-14.3589]), tensor([8.2222])]\n",
            "Logits for normal examples=[tensor([-14.3401]), tensor([8.3094])]\n",
            "Logits for normal examples=[tensor([-17.4374]), tensor([8.3915])]\n",
            "Logits for normal examples=[tensor([-14.3059]), tensor([8.4444])]\n",
            "Logits for normal examples=[tensor([-10.6653]), tensor([8.4939])]\n",
            "Logits for normal examples=[tensor([-17.5040]), tensor([8.5491])]\n",
            "Logits for normal examples=[tensor([-17.5213]), tensor([8.5997])]\n",
            "Logits for normal examples=[tensor([-17.5293]), tensor([8.6501])]\n",
            "Logits for normal examples=[tensor([-9.5574]), tensor([8.6944])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 7\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model_on_normal_examples(hidden_dim=20, lr=0.01, num_steps=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu87P74pMTvS",
        "outputId": "fb5d88f5-c589-4475-e183-28f0e3528ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for normal examples=[tensor([0.0710]), tensor([0.0687])]\n",
            "Logits for normal examples=[tensor([0.4350]), tensor([0.5277])]\n",
            "Logits for normal examples=[tensor([-0.1034]), tensor([-0.1034])]\n",
            "Logits for normal examples=[tensor([0.2924]), tensor([0.2924])]\n",
            "Logits for normal examples=[tensor([-2.5613]), tensor([-2.5123])]\n",
            "Logits for normal examples=[tensor([-0.0704]), tensor([-0.0703])]\n",
            "Logits for normal examples=[tensor([0.3971]), tensor([0.3971])]\n",
            "Logits for normal examples=[tensor([-1.6011]), tensor([1.9049])]\n",
            "Logits for normal examples=[tensor([0.5605]), tensor([0.5605])]\n",
            "Logits for normal examples=[tensor([-0.0688]), tensor([-0.0688])]\n",
            "Logits for normal examples=[tensor([0.0633]), tensor([0.0633])]\n",
            "Logits for normal examples=[tensor([-0.0113]), tensor([-0.0113])]\n",
            "Logits for normal examples=[tensor([0.0279]), tensor([0.0279])]\n",
            "Logits for normal examples=[tensor([0.0096]), tensor([0.0096])]\n",
            "Logits for normal examples=[tensor([0.0175]), tensor([0.0175])]\n",
            "Logits for normal examples=[tensor([0.0143]), tensor([0.0143])]\n",
            "Logits for normal examples=[tensor([0.0155]), tensor([0.0155])]\n",
            "Logits for normal examples=[tensor([0.0151]), tensor([0.0151])]\n",
            "Logits for normal examples=[tensor([0.0152]), tensor([0.0152])]\n",
            "Logits for normal examples=[tensor([0.0152]), tensor([0.0152])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 8\n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "train_model_on_normal_examples(hidden_dim=20, lr=0.01, num_steps=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqhAqGSAMQm8",
        "outputId": "822e8a82-3097-4ee4-e102-6ad5348755a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for normal examples=[tensor([0.0592]), tensor([0.0532])]\n",
            "Logits for normal examples=[tensor([0.2696]), tensor([0.2381])]\n",
            "Logits for normal examples=[tensor([-0.6512]), tensor([0.0539])]\n",
            "Logits for normal examples=[tensor([-0.1600]), tensor([-0.1600])]\n",
            "Logits for normal examples=[tensor([0.1757]), tensor([0.1757])]\n",
            "Logits for normal examples=[tensor([0.2338]), tensor([0.2338])]\n",
            "Logits for normal examples=[tensor([-0.2404]), tensor([-0.2404])]\n",
            "Logits for normal examples=[tensor([-0.0387]), tensor([-0.0387])]\n",
            "Logits for normal examples=[tensor([0.0368]), tensor([0.0368])]\n",
            "Logits for normal examples=[tensor([0.0225]), tensor([0.0225])]\n",
            "Logits for normal examples=[tensor([0.0103]), tensor([0.0103])]\n",
            "Logits for normal examples=[tensor([0.0371]), tensor([0.0334])]\n",
            "Logits for normal examples=[tensor([1.1129]), tensor([1.1129])]\n",
            "Logits for normal examples=[tensor([-4.6684]), tensor([0.7637])]\n",
            "Logits for normal examples=[tensor([-5.1905]), tensor([0.5639])]\n",
            "Logits for normal examples=[tensor([0.0511]), tensor([0.0511])]\n",
            "Logits for normal examples=[tensor([0.2257]), tensor([0.2257])]\n",
            "Logits for normal examples=[tensor([-0.9178]), tensor([-0.9178])]\n",
            "Logits for normal examples=[tensor([0.8536]), tensor([0.8536])]\n",
            "Logits for normal examples=[tensor([-0.0249]), tensor([-0.0249])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our RNN model works good for hard sequences of length <= 7 (for num_steps=2000)."
      ],
      "metadata": {
        "id": "Jmr8e4jX2bdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. Write a proper curricullum learning loop, where in a loop you consider longer and longer sequences, where expansion of the sequence length happens only after the model is trained successfully on the current length."
      ],
      "metadata": {
        "id": "e5I-t2CkloQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "\n",
        "# Pairs of (sequence, label)\n",
        "HARD_EXAMPLES = [([0.]+(SEQUENCE_LEN-1)*[1.], 0),\n",
        "                 (SEQUENCE_LEN*[1.], 1)]\n",
        "\n",
        "# Returns a sequence of ones or ones with a zero on a random index based on given label.\n",
        "def get_normal_sequence_of_given_length(label, length):\n",
        "    sequence = length*[1.]\n",
        "    if (label == 0):\n",
        "        sequence[random.randint(0, length-1)] = 0.\n",
        "    return sequence\n",
        "\n",
        "def eval_on_normal_examples(model, length):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "        for _, label in HARD_EXAMPLES:\n",
        "            sequence = get_normal_sequence_of_given_length(label, length)\n",
        "            input = torch.tensor(sequence).view(-1, 1, 1)\n",
        "            logit = model(input)\n",
        "            logits.append(logit.detach())\n",
        "        print(f'Logits for normal examples={logits}, score={abs(logits[1].item() - logits[0].item())}')\n",
        "        return abs(logits[1].item() - logits[0].item()) > 5\n",
        "\n",
        "\n",
        "def train_model_on_normal_examples(length, hidden_dim, lr, num_steps=10000, max_length=20, min_times=100):\n",
        "    model = Model(hidden_dim=hidden_dim)\n",
        "    loss_function = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.99)\n",
        "    times = 1\n",
        "\n",
        "    for step in range(num_steps):  \n",
        "      if step % 100 == 0:\n",
        "        if eval_on_normal_examples(model, length):\n",
        "          times += 1\n",
        "          if times == min_times:\n",
        "            times = 1\n",
        "            length += 1\n",
        "            print(f'The length is now {length}')\n",
        "            if length == max_length:\n",
        "              break\n",
        "\n",
        "      for _, label in HARD_EXAMPLES:\n",
        "        sequence = get_normal_sequence_of_given_length(label, length)\n",
        "        model.zero_grad()\n",
        "        logit = model(torch.tensor(sequence).view(-1, 1, 1))  \n",
        "              \n",
        "        loss = loss_function(logit.view(-1), torch.tensor([label], dtype=torch.float32))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()   "
      ],
      "metadata": {
        "id": "cUIIjxgp2mli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LEN = 2\n",
        "\n",
        "train_model_on_normal_examples(SEQUENCE_LEN, hidden_dim=20, lr=0.01, num_steps=10000, min_times=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgDTS73E4Gcq",
        "outputId": "21f0c773-bcc4-423c-ec6f-c7a58b514210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits for normal examples=[tensor([0.0174]), tensor([-0.0386])], score=0.056044748052954674\n",
            "Logits for normal examples=[tensor([-6.2102]), tensor([8.2213])], score=14.431459426879883\n",
            "Logits for normal examples=[tensor([-8.4256]), tensor([11.2166])], score=19.642135620117188\n",
            "Logits for normal examples=[tensor([-12.8428]), tensor([11.5330])], score=24.375752449035645\n",
            "Logits for normal examples=[tensor([-8.8468]), tensor([11.6134])], score=20.460264205932617\n",
            "Logits for normal examples=[tensor([-8.8820]), tensor([11.6574])], score=20.539387702941895\n",
            "Logits for normal examples=[tensor([-8.9113]), tensor([11.6930])], score=20.604293823242188\n",
            "Logits for normal examples=[tensor([-8.9393]), tensor([11.7243])], score=20.66359519958496\n",
            "Logits for normal examples=[tensor([-13.0060]), tensor([11.7498])], score=24.755763053894043\n",
            "Logits for normal examples=[tensor([-13.0232]), tensor([11.7806])], score=24.803720474243164\n",
            "The length is now 3\n",
            "Logits for normal examples=[tensor([-8.8884]), tensor([11.9346])], score=20.82295513153076\n",
            "Logits for normal examples=[tensor([-8.6647]), tensor([11.4399])], score=20.10460376739502\n",
            "Logits for normal examples=[tensor([-11.6730]), tensor([11.3404])], score=23.013370513916016\n",
            "Logits for normal examples=[tensor([-12.0189]), tensor([11.7331])], score=23.751962661743164\n",
            "Logits for normal examples=[tensor([-11.9467]), tensor([12.0862])], score=24.032901763916016\n",
            "Logits for normal examples=[tensor([-9.0013]), tensor([12.3290])], score=21.330327033996582\n",
            "Logits for normal examples=[tensor([-12.5876]), tensor([12.4946])], score=25.082274436950684\n",
            "Logits for normal examples=[tensor([-12.7204]), tensor([12.6215])], score=25.341879844665527\n",
            "Logits for normal examples=[tensor([-9.0730]), tensor([12.7314])], score=21.804372787475586\n",
            "The length is now 4\n",
            "Logits for normal examples=[tensor([-10.2352]), tensor([13.0025])], score=23.23770046234131\n",
            "Logits for normal examples=[tensor([-11.2806]), tensor([11.9672])], score=23.247857093811035\n",
            "Logits for normal examples=[tensor([-13.1516]), tensor([12.7407])], score=25.89231300354004\n",
            "Logits for normal examples=[tensor([-11.8576]), tensor([12.9119])], score=24.769521713256836\n",
            "Logits for normal examples=[tensor([-13.1996]), tensor([12.8869])], score=26.086495399475098\n",
            "Logits for normal examples=[tensor([-13.2106]), tensor([12.8732])], score=26.083749771118164\n",
            "Logits for normal examples=[tensor([-13.1867]), tensor([12.8643])], score=26.051015853881836\n",
            "Logits for normal examples=[tensor([-13.1949]), tensor([12.8581])], score=26.05298614501953\n",
            "Logits for normal examples=[tensor([-12.6007]), tensor([12.8523])], score=25.453022003173828\n",
            "The length is now 5\n",
            "Logits for normal examples=[tensor([-12.8386]), tensor([12.2663])], score=25.10487937927246\n",
            "Logits for normal examples=[tensor([-13.3279]), tensor([12.6436])], score=25.971582412719727\n",
            "Logits for normal examples=[tensor([-13.0601]), tensor([12.8023])], score=25.862406730651855\n",
            "Logits for normal examples=[tensor([-13.2663]), tensor([12.8467])], score=26.11296558380127\n",
            "Logits for normal examples=[tensor([-9.2908]), tensor([12.9391])], score=22.229934692382812\n",
            "Logits for normal examples=[tensor([-13.2796]), tensor([12.9260])], score=26.20552635192871\n",
            "Logits for normal examples=[tensor([-13.3614]), tensor([12.9321])], score=26.293441772460938\n",
            "Logits for normal examples=[tensor([-12.8265]), tensor([12.9271])], score=25.753671646118164\n",
            "Logits for normal examples=[tensor([-12.9204]), tensor([12.9393])], score=25.859703063964844\n",
            "The length is now 6\n",
            "Logits for normal examples=[tensor([-13.1188]), tensor([12.7491])], score=25.867923736572266\n",
            "Logits for normal examples=[tensor([-12.0585]), tensor([13.0399])], score=25.09843635559082\n",
            "Logits for normal examples=[tensor([-13.1367]), tensor([12.9599])], score=26.09659194946289\n",
            "Logits for normal examples=[tensor([-12.6697]), tensor([12.9790])], score=25.64864158630371\n",
            "Logits for normal examples=[tensor([-13.1482]), tensor([12.9672])], score=26.11536979675293\n",
            "Logits for normal examples=[tensor([-13.4032]), tensor([12.9630])], score=26.36617374420166\n",
            "Logits for normal examples=[tensor([-13.3132]), tensor([12.9629])], score=26.276138305664062\n",
            "Logits for normal examples=[tensor([-9.3754]), tensor([12.9651])], score=22.34052085876465\n",
            "Logits for normal examples=[tensor([-13.1700]), tensor([12.9668])], score=26.136807441711426\n",
            "The length is now 7\n",
            "Logits for normal examples=[tensor([-13.4387]), tensor([12.9317])], score=26.37039089202881\n",
            "Logits for normal examples=[tensor([-13.1250]), tensor([12.9660])], score=26.09100341796875\n",
            "Logits for normal examples=[tensor([-13.1580]), tensor([12.9740])], score=26.132018089294434\n",
            "Logits for normal examples=[tensor([-13.3461]), tensor([12.9803])], score=26.326333045959473\n",
            "Logits for normal examples=[tensor([-9.4019]), tensor([13.0077])], score=22.40963363647461\n",
            "Logits for normal examples=[tensor([-13.4660]), tensor([12.9433])], score=26.409247398376465\n",
            "Logits for normal examples=[tensor([-13.1769]), tensor([12.9849])], score=26.16180419921875\n",
            "Logits for normal examples=[tensor([-13.2113]), tensor([13.0014])], score=26.21272850036621\n",
            "Logits for normal examples=[tensor([-13.3652]), tensor([13.0204])], score=26.385571479797363\n",
            "The length is now 8\n",
            "Logits for normal examples=[tensor([-13.4934]), tensor([13.0270])], score=26.52034568786621\n",
            "Logits for normal examples=[tensor([-9.4524]), tensor([12.7901])], score=22.242560386657715\n",
            "Logits for normal examples=[tensor([-13.2020]), tensor([12.9774])], score=26.179438591003418\n",
            "Logits for normal examples=[tensor([-13.5033]), tensor([12.9582])], score=26.461566925048828\n",
            "Logits for normal examples=[tensor([-13.3865]), tensor([13.0289])], score=26.415409088134766\n",
            "Logits for normal examples=[tensor([-13.1950]), tensor([12.9657])], score=26.16068458557129\n",
            "Logits for normal examples=[tensor([-13.5173]), tensor([12.9932])], score=26.510488510131836\n",
            "Logits for normal examples=[tensor([-13.2204]), tensor([13.0405])], score=26.260881423950195\n",
            "Logits for normal examples=[tensor([-9.4702]), tensor([13.0132])], score=22.4833927154541\n",
            "The length is now 9\n",
            "Logits for normal examples=[tensor([-13.2155]), tensor([13.0469])], score=26.26246738433838\n",
            "Logits for normal examples=[tensor([-13.2259]), tensor([13.0333])], score=26.25912857055664\n",
            "Logits for normal examples=[tensor([-13.2302]), tensor([13.0240])], score=26.25415802001953\n",
            "Logits for normal examples=[tensor([-13.5402]), tensor([12.9776])], score=26.517773628234863\n",
            "Logits for normal examples=[tensor([-13.2361]), tensor([13.0657])], score=26.3018159866333\n",
            "Logits for normal examples=[tensor([-13.2713]), tensor([13.0776])], score=26.348881721496582\n",
            "Logits for normal examples=[tensor([-13.2300]), tensor([12.9735])], score=26.20351791381836\n",
            "Logits for normal examples=[tensor([-13.2777]), tensor([13.0176])], score=26.295337677001953\n",
            "Logits for normal examples=[tensor([-13.2472]), tensor([13.0748])], score=26.32201862335205\n",
            "The length is now 10\n",
            "Logits for normal examples=[tensor([-12.5533]), tensor([13.0527])], score=25.6060791015625\n",
            "Logits for normal examples=[tensor([-13.2532]), tensor([13.0332])], score=26.286405563354492\n",
            "Logits for normal examples=[tensor([-13.2925]), tensor([13.1190])], score=26.411502838134766\n",
            "Logits for normal examples=[tensor([-13.2712]), tensor([13.0633])], score=26.334463119506836\n",
            "Logits for normal examples=[tensor([-13.2518]), tensor([13.0578])], score=26.30954074859619\n",
            "Logits for normal examples=[tensor([-13.2549]), tensor([13.0561])], score=26.311054229736328\n",
            "Logits for normal examples=[tensor([-13.2726]), tensor([13.1081])], score=26.380786895751953\n",
            "Logits for normal examples=[tensor([-13.2839]), tensor([12.9888])], score=26.27273464202881\n",
            "Logits for normal examples=[tensor([-9.5313]), tensor([13.0777])], score=22.608951568603516\n",
            "The length is now 11\n",
            "Logits for normal examples=[tensor([-13.2905]), tensor([13.1115])], score=26.401992797851562\n",
            "Logits for normal examples=[tensor([-13.3184]), tensor([13.1217])], score=26.44013023376465\n",
            "Logits for normal examples=[tensor([-9.5359]), tensor([13.1055])], score=22.641364097595215\n",
            "Logits for normal examples=[tensor([-13.2977]), tensor([12.9818])], score=26.279483795166016\n",
            "Logits for normal examples=[tensor([-13.6070]), tensor([13.0349])], score=26.641942024230957\n",
            "Logits for normal examples=[tensor([-13.3030]), tensor([13.1857])], score=26.48867702484131\n",
            "Logits for normal examples=[tensor([-13.3309]), tensor([13.1108])], score=26.441694259643555\n",
            "Logits for normal examples=[tensor([-13.3336]), tensor([13.0505])], score=26.3841495513916\n",
            "Logits for normal examples=[tensor([-13.3361]), tensor([13.0610])], score=26.39703941345215\n",
            "The length is now 12\n",
            "Logits for normal examples=[tensor([-13.4844]), tensor([13.1651])], score=26.64957904815674\n",
            "Logits for normal examples=[tensor([-13.3193]), tensor([13.1662])], score=26.485478401184082\n",
            "Logits for normal examples=[tensor([-13.3182]), tensor([13.2045])], score=26.52260971069336\n",
            "Logits for normal examples=[tensor([-13.3210]), tensor([13.0227])], score=26.343745231628418\n",
            "Logits for normal examples=[tensor([-13.2825]), tensor([13.1559])], score=26.438410758972168\n",
            "Logits for normal examples=[tensor([-13.4957]), tensor([13.0083])], score=26.503992080688477\n",
            "Logits for normal examples=[tensor([-13.3256]), tensor([13.2190])], score=26.544522285461426\n",
            "Logits for normal examples=[tensor([-13.3331]), tensor([13.1736])], score=26.506736755371094\n",
            "Logits for normal examples=[tensor([-13.3576]), tensor([13.1078])], score=26.46531581878662\n",
            "The length is now 13\n"
          ]
        }
      ]
    }
  ]
}